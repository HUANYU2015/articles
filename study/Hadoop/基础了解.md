# å¤§æ•°æ®åŸºç¡€

## å¤§æ•°æ®å¸¦æ¥çš„æŠ€æœ¯é©±åŠ¨

* æŠ€æœ¯é©±åŠ¨ï¼šæ•°æ®é‡å¤§
  * å­˜å‚¨ï¼šæ–‡ä»¶å­˜å‚¨ ==> åˆ†å¸ƒå¼å­˜å‚¨
  * è®¡ç®—ï¼šå•æœº ==> åˆ†å¸ƒå¼è®¡ç®— MapReduce
  * ç½‘ç»œï¼šä¸‡å…†å¸¦å®½
  * DBï¼šRDBMS ==> NoSQL(HBase/Redis...)
* å•†ä¸šé©±åŠ¨ï¼šä»·å€¼é©±åŠ¨

## å¤§æ•°æ®ç°å­˜çš„æ¨¡å¼

* æ‹¥æ•°æ®ï¼Œæ²¡æœ‰å¤§æ•°æ®æ€ç»´
* æ²¡æœ‰å¤§æ•°æ®ï¼Œæœ‰å¤§æ•°æ®æ€ç»´
* æ—¢æœ‰å¤§æ•°æ®ï¼Œåˆæœ‰å¤§æ•°æ®æ€ç»´

## å¤§æ•°æ®çš„æŠ€æœ¯æ¦‚å¿µ

* å•æœºï¼šCPU memory disk
* åˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—/å¤„ç†

$è¿‡ç¨‹$

* æ•°æ®é‡‡é›†ï¼ˆç²¾ç»†åŒ–ç­›é€‰ï¼‰ï¼šflume Sqoop
* æ•°æ®å­˜å‚¨ï¼šHadoop
* æ•°æ®å¤„ç†/åˆ†æ/æŒ–æ˜ï¼šHadoopã€Sparkã€Flink...
* å¯è§†åŒ–ï¼š

## å¤§æ•°æ®æŠ€æœ¯æ¶æ„

* å¯¹ç°æœ‰æ•°æ®åº“ç®¡ç†æŠ€æœ¯çš„æŒ‘æˆ˜ï¼šMySQL->NoSQL
* ç»å…¸çš„æ•°æ®åº“æŠ€æœ¯å¹¶æ²¡æœ‰è€ƒè™‘åˆ°æ•°æ®çš„å¤šç±»å‹
* å®æ—¶æ€§çš„æŠ€æœ¯æŒ‘æˆ˜
* ç½‘ç»œæ¶æ„ã€æ•°æ®ä¸­å¿ƒã€è¿ç»´çš„æŒ‘æˆ˜
* éšç§é˜²æ³„æ¼
* æ•°æ®æºå¤æ‚å¤šæ ·çš„æŒ‘æˆ˜

## å¦‚ä½•å¯¹å¤§æ•°æ®è¿›è¡Œå­˜å‚¨å’Œåˆ†æ

|ç³»ç»Ÿç“¶é¢ˆ|Google å¤§æ•°æ®æŠ€æœ¯|
|:---:|:---:|
|å­˜å‚¨å®¹é‡|MapReduce|
|è¯»å†™é€Ÿåº¦|bigtable|
|è®¡ç®—æ•ˆç‡|GFS|

## å¤§æ•°æ®å…¸å‹åº”ç”¨

Countã€Sumã€AVG -> group by/join -> çª—å£åˆ†æå‡½æ•° -> å¼‚å¸¸ã€æ¬ºè¯ˆæ£€æµ‹ -> äººå·¥æ™ºèƒ½

æŠ¥è¡¨ -> ç”¨æˆ·ç»†åˆ† -> æŒ‡æ ‡ç›‘æ§ -> æŒ‡æ ‡é¢„è­¦

## åˆè¯†Hadoop

### Hadoop æ¦‚è¿°

logo æ¥æºäºåŠ¨ç‰©ï¼ŒHadoop ä¹‹çˆ¶æ˜¯ Doug Cuttingã€‚
Hadoop æ˜¯ Apache ç¤¾åŒºçš„é¡¶çº§é¡¹ç›®ï¼šxxxx.apache.org
hadoop hive hbase spark flink storm

The Apache hadoop project develops open-source software for `reliable, scalable, distributed computing`.

The Apache Hadoop software libary is a framework that allows for the `distributed processing` of large data sets across clusters of computers using simple programming models.

Hadoop æä¾›`åˆ†å¸ƒå¼å­˜å‚¨ + åˆ†å¸ƒå¼è®¡ç®—`ï¼Œæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼çš„ç³»ç»ŸåŸºç¡€æ¶æ„ã€‚

åˆ†å¸ƒå¼å­˜å‚¨ï¼šä¸€ä¸ªæ–‡ä»¶è¢«æ‹†åˆ†æˆå¾ˆå¤šå—ï¼Œå¹¶ä¸”ä»¥å‰¯æœ¬çš„æ–¹å¼å­˜å‚¨åœ¨å„ä¸ªèŠ‚ç‚¹ä¸­ã€‚
åˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„ï¼šç”¨æˆ·å¯ä»¥åœ¨ä¸äº†è§£åˆ†å¸ƒå¼åº•å±‚ç»†èŠ‚çš„å‰æä¸‹ä½¿ç”¨ã€‚

### Hadoop æ ¸å¿ƒç»„ä»¶

* **Hadoop Common**: The common utilities that support the other Hadoop modules.
* **Hadoop Distributed File System(HDFS-åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ)**: A distributed file system that provides high-throughput access to application data.
* **Hapoop Yarn(åˆ†å¸ƒå¼èµ„æºè°ƒåº¦æ¡†æ¶)**: A framework for `job scheduling` and `cluster resource management`
* **Hadoop MapReduce(åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶)**: A Yarn-based system for parallel processing of large data sets
* **Hadoop Ozone(å¯¹è±¡å­˜å‚¨)**: A object store for Hadoop

#### HDFS-åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ

* æºäº Google çš„ gfs è®ºæ–‡ï¼Œè®ºæ–‡å‘è¡¨ä¸2003å¹´10æœˆ
* hdfs æ˜¯ gfs çš„å…‹éš†ç‰ˆ
* HDFS ç‰¹ç‚¹ï¼šæ‰©å±•æ€§ & å®¹é”™æ€§ & æµ·é‡æ•°æ®å­˜å‚¨
* å°†æ–‡ä»¶åˆ‡åˆ†æˆæŒ‡å®šå¤§å°(block-size)çš„`æ•°æ®å—(block)`ï¼Œå¹¶ä»¥`å¤šå‰¯æœ¬(multi-standy)`çš„æ–¹å¼å­˜å‚¨åœ¨æœºå™¨ä¸Šé¢
* æ•°æ®åˆ‡åˆ†ã€å¤šå‰¯æœ¬ã€å®¹é”™ç­‰æ“ä½œå¯¹ç”¨æˆ·æ˜¯é€æ˜çš„(ç”¨æˆ·æ— éœ€çŸ¥æ™“)

$æ¦‚è¿°$

The hdfs is a distributed file system designed to run on commodity hardware. It has many simikarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX(å¯ç§»æ¤æ€§æ“ä½œç³»ç»Ÿæ¥å£) requirements to enable streaming access to file system data.

1. distributed
2. commodity hardware or low-cost hardware
3. fault-tolerant
4. high throughput access
5. large data sets

æ™®é€šæ–‡ä»¶ç³»ç»Ÿ vs åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ

    å•æœº
    åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿèƒ½å¤Ÿæ¨ªè·¨ N ä¸ªæœºå™¨

$HDFSå‰æå’Œè®¾è®¡ç›®æ ‡$

* **Hardware Failure**: Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system's data. The fact that there are a huge number of components and that each component has a **`non-trivial probability of failure`** means that some component of HDFS is always non-functional(å§‹ç»ˆä¸èµ·ä½œç”¨). Therefore, **`detection of faults and quick, automatic recovery from them`** is a core architectural goal of HDFS.

* **Streaming Data Access**: Applications that run on HDFS need **`streaming access`** to their data sets. They are not general purpose applications that typically run on general purpose file systems. HDFS is designed more for **`batch processing`** rather than **`interactive use`** by users. The emphasis is on **`high throughput of data access`** rather than **`low latency of data access`**. POSIX(å¯ç§»æ¤æ€§æ“ä½œç³»ç»Ÿæ¥å£) imposes many hard requirements that are not needed for applications that are targeted for HDFS. POSIX semantics in a few areas has been traded to increase data throughput rates.

* **Large Data Sets**: Application that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned(è°ƒä¼˜çš„) to support large files. It should provides **`high aggregate data bandwidth(é«˜èšåˆæ•°æ®å¸¦å®½)`** and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.

* **Simple Coherency Model**: ç®€åŒ–çš„æ•°æ®ä¸€è‡´æ€§é—®é¢˜ï¼Œå¹¶å®ç°äº†é«˜ååé‡æ•°æ®è®¿é—®ã€‚

* **Moving Computation is Cheaper than Moving Data**: A computation requested by an application is much more efficient if **`it is executed near the data it operates on`**. This is especially true when the size of the data set is huge. This minizes network congestion and increase the overall throughput of system. The assumption is that it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. `HDFS provides interfaces for application to move themselves closer to where the data is located`.

* **Portability(å¯ç§»æ¤æ€§ã€ä¾¿æºæ€§) Across Heterogeneous Hardware and Software Platforms**: HDFS has been designed to be portable from one platform to another. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.

#### åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶-MapReduce

* æºäº Google çš„ MapReduce è®ºæ–‡ï¼Œè®ºæ–‡å‘è¡¨äº2004å¹´12æœˆ
* MapReduce æ˜¯ Google MapReduce çš„å…‹éš†ç‰ˆ
* MapReduce ç‰¹ç‚¹ï¼šæ‰©å±•æ€§ & å®¹é”™æ€§ & æµ·é‡æ•°æ®ç¦»çº¿å¤„ç†
* word count process: input -> splitting -> mapping -> shuffling -> reducing -> final result

#### èµ„æºè°ƒåº¦ç³»ç»Ÿ YARN

* YARNï¼šyet another resource negotiator
* è´Ÿè´£æ•´ä¸ª`é›†ç¾¤èµ„æº`çš„ç®¡ç†å’Œè°ƒåº¦
* YARN ç‰¹ç‚¹ï¼šæ‰©å±•æ€§ & å®¹é”™æ€§ & `å¤šæ¡†æ¶èµ„æº`ç»Ÿä¸€è°ƒåº¦
* å¤šæ¡†æ¶ï¼šscript-Pig sql-hive nosql-hbase stream-storm in-memory-spark search-solr flink

### Hadoop ä¼˜åŠ¿

$é«˜å¯é æ€§$

* æ•°æ®å­˜å‚¨ï¼šæ•°æ®åº“å¤šå‰¯æœ¬
* æ•°æ®è®¡ç®—ï¼šé‡æ–°è°ƒåº¦ä½œä¸šè®¡ç®—

$é«˜æ‰©å±•æ€§$

* å­˜å‚¨ã€è®¡ç®—èµ„æºä¸å¤Ÿæ—¶ï¼Œå¯ä»¥æ¨ªå‘çº¿å‹æ‰©å±•æœºå™¨
* ä¸€ä¸ªé›†ç¾¤ä¸­å¯ä»¥åŒ…å«æ•°ä»¥åƒè®¡çš„èŠ‚ç‚¹
* å­˜å‚¨åœ¨å»‰ä»·çš„æœºå™¨ä¸Š(å» IOE)ï¼Œé™ä½æˆæœ¬
* æˆç†Ÿçš„ç”Ÿæ€åœˆ

### Hadoop å‘å±•å²

è¯ç”Ÿäº2006å¹´ï¼Œnutch-gfs-mapreduce-bigtable(hbase)-2007ç™¾åº¦ç§»åŠ¨ä½¿ç”¨-hive-cloudera-cbh-spark-

### Hadoop ç”Ÿæ€ç³»ç»Ÿ

**ç‹­ä¹‰çš„ Hadoop**ï¼šæ˜¯ä¸€ä¸ªé€‚ç”¨äºå¤§æ•°æ®åˆ†å¸ƒå¼å­˜å‚¨ï¼ˆHDFSï¼‰ã€åˆ†å¸ƒå¼è®¡ç®—ï¼ˆMapReduceï¼‰å’Œèµ„æºè°ƒåº¦ï¼ˆYARNï¼‰çš„å¹³å°ï¼›

**å¹¿ä¹‰çš„ Hadoop**ï¼šæŒ‡çš„æ˜¯ `Hadoop çš„ç”Ÿæ€ç³»ç»Ÿ`ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¾ˆåºå¤§çš„æ¦‚å¿µï¼ŒHadoop æ˜¯å…¶ä¸­æœ€é‡è¦æœ€åŸºç¡€çš„ä¸€ä¸ªéƒ¨åˆ†ï¼›ç”Ÿæ€ç³»ç»Ÿä¸­çš„ä¸€ä¸ªå­ç³»ç»Ÿ`åªè§£å†³æŸä¸€ç‰¹å®šé—®é¢˜åŸŸ`ï¼ˆç”šè‡³å¯èƒ½å¾ˆçª„ï¼‰ä¸åšç»Ÿä¸€å‹çš„ä¸€ä¸ªå…¨èƒ½ç³»ç»Ÿï¼Œè€Œæ˜¯`å°è€Œç²¾`çš„`å¤šä¸ªå°çš„ç³»ç»Ÿ`ï¼›

![Xnip2019-04-24_17-33-22](/assets/Xnip2019-04-24_17-33-22.png)

* å¼€æºã€ç¤¾åŒºæ´»è·ƒ
* å›Šæ‹¬äº†å¤§æ•°æ®å¤„ç†çš„æ–¹æ–¹é¢é¢
* æˆç†Ÿçš„ç”Ÿæ€åœˆ

### Hadoop å‘è¡Œç‰ˆçš„é€‰æ‹©

* Apache
  * ä¼˜ç‚¹ï¼šçº¯å¼€æºï¼Œ
  * ç¼ºç‚¹ï¼šä¸åŒç‰ˆæœ¬ã€ä¸åŒæ¡†æ¶ä¹‹é—´æ•´åˆé—®é¢˜å¤š jar å†²çª...
* CDH
  * ä¼˜ç‚¹ï¼šhttp://www.cloudera.com cmï¼ˆclouder managerï¼‰é€šè¿‡é¡µé¢ä¸€é”®å®‰è£…å„ç§æ¡†æ¶ã€è¿›è¡Œå‡çº§æ“ä½œï¼Œ
  * ç¼ºç‚¹ï¼šcm ä¸å¼€æº
* Hortonworks: HDP ä¼ä¸šé€šè¿‡å‘å¸ƒè‡ªå·±çš„æ•°æ®å¹³å°å¯ä»¥ç›´æ¥åŸºäºé¡µé¢æ¡†æ¶è¿›è¡Œæ”¹é€ 
  * ä¼˜ç‚¹ï¼šåŸè£… Hadoopã€çº¯å¼€æºã€æ”¯æŒ tez(åˆ†å¸ƒå¼dhæ¡†æ¶)
  * ç¼ºç‚¹ï¼šä¼ä¸šçº§å®‰å…¨ä¸å¼€æºï¼Œ
* MapReduce

## HDFS Architecture

* NameNode and DataNodes
* master/slave architecture

### æ¶æ„æ¦‚è¿°

HDFS has a **`master/slave`** architecture. An HDFS cluster consists of a single NameNode, a master server that **`manages the file system namespace`** and **`regulates access to files by clients`**.

In addition, there are a number of DataNodes, usually one per node in the cluster, which **`manage storage attached to the nodes`** that they are run on.

HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file split into one or more blocks and these blocks are stored in a set of DataNodes(ä¸€ç»„ DataNode).

**`The NameNode executes file system namespace operations like opening, closing, and renaming files and directories`**. It also determines the mapping of blocks to DataNodes.

**`The DataNodes are responsible for serving read and write requests from the file system's clients. The DataNodes also perform block creationã€deletionã€and replication upon instruction from the NameNode`**.

A typical deployment has a dedicated(ä¸“æœ‰çš„) machine that runs only **`the NameNode software`**. Each of the other machines in the cluster runs **`one instance of the DataNode software`**. The architecture does not preclude(æ’æ–¥) running multiple DataNodes on the same machine but in a real deployment that is rarely the case.

### The File System Namespace

HDFS supports a traditional hierarchical(å±‚çº§çš„) file organization. A user or an application can create directories and store files inside these directories. **`The file system namespace hierarchy is similar to most other existing file systems`**; one can create and remove files, move a file from one directory to another, or rename a file. HDFS supports **user quotas** and **access permissions**. HDFS does not support hard links or soft links. However, the HDFS architecture does not preclude implementing these features.

**`The NameNode maintains the file system namespace. Any change to the file system namespace or its properties is recorded by the HDFS`**. An application can specify the number of replicas of a file that should be maintained by HDFS. The number of copies of a file is called the **replication factor** of that file. This information is stored by the NameNode.

### Data Replication

HDFS is designed to reliably store very **large files** across machines in a large cluster. It stores each file as a sequence of blocks. **The blocks of file are replicated for fault tolerance**. The block size and replication factor are configurable per file.

All blocks in a file except the last block are the same size, while users can start a new block without filling out the last block to the configured block size after the support for variable length block was added to append and hsync.(åŒæ—¶åœ¨å°†å¯¹å¯å˜é•¿åº¦ block çš„æ”¯æŒæ·»åŠ åˆ°â€œappendâ€å’Œâ€œhsyncâ€œåŠŸèƒ½ä¸­åï¼Œç”¨æˆ·å¯ä»¥ä»¥ä¸€ä¸ªæ–°çš„ block å¼€å§‹è€Œä¸å¿…å°†åŸæœ‰æœ€åä¸€ä¸ªblock å¡«æ»¡è‡³é…ç½®çš„å®¹é‡è§„æ ¼)

An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once(except for appends and truncates é™¤è¿‡è¿½åŠ å’Œæˆªæ–­) and have strictly(ä¸¥æ ¼åœ°ã€ç¡®å®åœ°) one writer at any time.

The NameNode makes all decisions regarding replication of blocks. It periodically receives a **Heartbeat** and a **Blockreport** from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode.

![hdfsdatanodes](/assets/hdfsdatanodes.png)

## linux æ“ä½œ

linux ç”¨æˆ·åï¼šhadoop
å¯†ç ï¼š123456
HostName: hadoop000

åˆ›å»ºæ–‡ä»¶å¤¹ï¼š
    mkdir software è½¯ä»¶å®‰è£…åŒ…å­˜æ”¾ä½ç½®
    mkdir app      è½¯ä»¶å®‰è£…ä½ç½®
    mkdir data     æ•°æ®å­˜æ”¾ä½ç½®
    mkdir lib      å­˜æ”¾ä½œä¸š jar
    mkdir shell    å­˜æ”¾ç›¸å…³è„šæœ¬
    mkdir maven_resp å­˜æ”¾ mavenä¾èµ–åŒ…
hadoop ç”¨æˆ·è½¬ root ç”¨æˆ·ï¼š
    ```sudo -i```
åå‘
    ```su hadoop```
linux ç‰ˆæœ¬ï¼šcentos7
    ```uname -a```

é…ç½®å¥½ IP åŠåŸŸåæ˜ å°„ï¼š
    ```vim /etc/hosts```
    server ip   hostname

## Hadoop åˆå§‹å®‰è£…é…ç½®

ä¸‹è½½åœ°å€ï¼šhttp://archive.cloudera.com/cdh5/cdh/5/
Hadoop é€‰æ‹©hadoop-2.6.0-cdh5.15.1.tar.gz

ä½¿ç”¨å•æœºç‰ˆè¿›é¡¹æ¡†æ¶å­¦ä¹ 

Hadoop å®‰è£…å‰ç½®è¦æ±‚å®‰è£…ä»¥ä¸‹è½¯ä»¶

* Java 1.8+
* ssh

### æ‹·è´æ–‡ä»¶åˆ°æœåŠ¡å™¨

``` bash
scp hadoop-2.6.0-cdh5.15.1 hadoop@10.211.55.8:~/software/
```

### å®‰è£…ï¼šè§£å‹ç¼©åŠç¯å¢ƒé…ç½®

$è§£å‹ç¼©è‡³ `~/app/`$

``` bash
tar -zxvf hadoop-2.6.0-cdh5.15.1 -C ~/app/
```

hadoop è½¯ä»¶åŒ…å¸¸ç”¨ç›®å½•åå•
    bin: hadoop å®¢æˆ·ç«¯åå•
    etc/hadoopï¼šhadoopç›¸å…³çš„é…ç½®æ–‡ä»¶å­˜æ”¾ç›®å½•
    sbin: å¯åŠ¨ hadoopç›¸å…³è¿›ç¨‹çš„è„šæœ¬

$é…ç½® profile æ–‡ä»¶$

``` bash
# æ‰“å¼€ profile æ–‡ä»¶
vim ~/etc/profile
# æ·»åŠ  HADOOP_HOMEï¼Œå¹¶ä¿®æ”¹ `PATH`ï¼Œå…·ä½“å¦‚ä¸‹
export HADOOP_HOME=/usr/hadoop/app/hadoop-2.6.0-cdh5.15.1
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
# æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æ˜¯ profile ä¿®æ”¹ç”Ÿæ•ˆ
source profile
```

$é…ç½® Hadoop çš„é…ç½®æ–‡ä»¶hadoop-env.sh$

``` bash
# æ‰“å¼€ hadoop-env.sh æ–‡ä»¶```
# æ³¨æ„åœ¨ä½¿ç”¨ `cd /etc` å‘½ä»¤æ—¶ï¼Œæ€»æ˜¯è¿›å…¥ç³»ç»Ÿè‡ªå¸¦ `etc`ç›®å½•ä¸­
vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh
# æŸ¥æ‰¾æˆ–æ·»åŠ ä¸‹å¥ï¼Œ
# æ³¨æ„è¯¥æ–‡ä»¶ä¸­ `JAVA_HOME` çš„å€¼åŒ…å« `${JAVA_HOME}` å¯èƒ½æ— æ•ˆ ï¼Œå»ºè®®ä½¿ç”¨æ˜æ–‡è·¯å¾„
export JAVA_HOME=/usr/java/jdk1.8.0_152
```

### åˆ†å¸ƒå¼éƒ¨ç½²

æœ¬æ¬¡æµ‹è¯•ä½¿ç”¨ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²æ–¹å¼ï¼ˆPseudo-Distributed Operationï¼‰

$é…ç½®core-site.xml$

``` bash
# æ‰“å¼€ `core-site.xml` æ–‡ä»¶
vim $HADOOP_HOME/etc/hadoop/core-site.xml
# åœ¨ <configuration> èŠ‚ç‚¹ä¸­æ´ªæ·»åŠ å¦‚ä¸‹ä»£ç 
# æ³¨æ„ä¸‹é¢çš„ `hadoop000` ä¸º hostnameï¼Œéœ€åœ¨ï¼ˆæœ¬åœ°åŠæœåŠ¡ç«¯ï¼‰ `~/etc/hosts` æ–‡ä»¶ä¸­é…ç½®ï¼Œå³åŠ å…¥ `10.211.55.8 hadoop000`ï¼Œå¹¶ä¸”åº”ä¿è¯ç«¯å£ 9000 ä¸è¢«å ç”¨
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://hadoop000:9000</value>
</property>
```

$é…ç½®hdfs-site.xml$

``` bash
# æ‰“å¼€ `hdfs-site.xml` æ–‡ä»¶
vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml
# åœ¨ <configuration> èŠ‚ç‚¹ä¸­æ´ªæ·»åŠ å¦‚ä¸‹ä»£ç 
# æ³¨æ„éœ€åœ¨ `$HADOOP_HOME` ä¸­åˆ›å»º `tmp` ç›®å½•
# ä¸” `hadoop.tmp.dir` å¯¹åº”çš„ URI ä¸èƒ½åŒ…å« `${}` ç±»å‹ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™å¯¼è‡´é…ç½®æ— æ•ˆå‡ºé”™ ï¼Œå»ºè®®ä½¿ç”¨æ˜æ–‡è·¯å¾„
<property>
    <name>dfs.replication</name>
    <value>1</value>
</property>
<property>
    <name>hadoop.tmp.dir</name>
    <value>/usr/hadoop/app/hadoop-2.6.0-cdh5.15.1/tmp</value>
</property>
```

$é…ç½®slaves$

``` bash
# æ‰“å¼€ `slaves` æ–‡ä»¶ï¼Œæ·»åŠ  `hadoop000`
vim $HADOOP_HOME/etc/hadoop/slaves
```

### å¯åŠ¨ HDFS

$æ ¼å¼åŒ–æ–‡ä»¶ç³»ç»Ÿ$

``` bash
# åœ¨ä»»æ„ç›®å½•ä¸‹æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œæ³¨æ„ `hdfs` å‘½ä»¤åœ¨ Hadoop çš„ bin ç›®å½•ä¸‹
hdfs namenode -format
```

$å¯åŠ¨ namenode å’Œ datanodeï¼ˆStart NameNode daemon and DataNode daemonï¼‰$

``` bash
# æ³¨æ„ä¸‹é¢çš„å‘½ä»¤éœ€åœ¨ hadoop çš„ `sbin` ç›®å½•ä¸‹æ‰§è¡Œ
cd $HADOOP_HOME/bin
start-dfs.sh
```

> ğŸ¦‹æ³¨æ„ï¼šstart-dfs.sh = hadoop-daemons.sh start namenode
>                       hadoop-daemons.sh start datanode
>                       hadoop-daemons.sh start secondarynamenode

$éªŒè¯$

ä½¿ç”¨ jps å‘½ä»¤è¿›è¡ŒéªŒè¯ æˆ–è€…ä½¿ç”¨ åœ¨æµè§ˆå™¨åœ°å€æ é”®å…¥ http://10.211.55.8:50070ï¼Œè¿›è¡ŒéªŒè¯ã€‚
å¦‚ä½¿ç”¨åä¸€ç§æ–¹æ³•éªŒè¯ä¸æˆåŠŸï¼Œåˆ™éœ€å…³é—­é˜²ç«å¢™

``` bash
# æ£€æŸ¥é˜²ç«å¢™çŠ¶æ€
firewall-cmd --state
systemctl status firewalld
# ç¦ç”¨æˆ–å…³é—­é˜²ç«å¢™
systemctl stop firewalld
```

$åœæ­¢è¿›ç¨‹(namenode/datanode/secondarynamenode)$

``` bash
# æ³¨æ„ä¸‹é¢çš„å‘½ä»¤éœ€åœ¨ hadoop çš„ `sbin` ç›®å½•ä¸‹æ‰§è¡Œ
cd $HADOOP_HOME/bin
stop-dfs.sh
```

> ğŸ¦‹æ³¨æ„ï¼šä¹Ÿå¯ä»¥ä½¿ç”¨ `hadoop-daemons.sh stop + daemons åç§°` å•ç‹¬æ‰§è¡Œåœæ­¢æ“ä½œ

## hadoop å­˜å‚¨æœºåˆ¶

$å¤åˆ¶æ–‡ä»¶åˆ°åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ$

``` bash
# å¤åˆ¶æ–‡ä»¶åˆ°åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿçš„ä¸»ç›®å½•ä¸­
hdfs dfs -put hadoop-2.6.0-cdh5.15.1.tar.gz /
# æŸ¥çœ‹åˆ†å¸ƒå¼ç³»ç»Ÿä¸»ç›®å½•ä¸­å­˜æœ‰çš„æ–‡ä»¶ï¼Œâ•å‚æ•° R ä¸ºé€’å½’æŸ¥è¯¢
hdfs dfs -ls /
hdfs dfs -ls -R /
```

$æµè§ˆå™¨åŠç³»ç»Ÿä¸­æŸ¥çœ‹æ–‡ä»¶å­˜å‚¨æœºåˆ¶$

åœ¨æµè§ˆå™¨åœ°å€æ é”®å…¥http://10.211.55.8:50070ï¼Œå¹¶ç‚¹å‡» `Utilities >> Browse the file system` æŸ¥çœ‹ç›¸å…³ `block` ä¿¡æ¯ã€‚

åœ¨ $HADOOP_HOME/tmp/dfs/data/current/BP-1788419893-10.211.55.8-1557305523480/current/finalized/subdir0/subdir0 æŸ¥çœ‹å­˜å‚¨åœ¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä¸­çš„åˆ†å—æ–‡ä»¶ã€‚

``` bash
-rw-rw-r--. 1 hadoop hadoop 134217728 5æœˆ   9 09:51 blk_1073741825
-rw-rw-r--. 1 hadoop hadoop   1048583 5æœˆ   9 09:51 blk_1073741825_1001.meta
-rw-rw-r--. 1 hadoop hadoop 134217728 5æœˆ   9 09:52 blk_1073741826
-rw-rw-r--. 1 hadoop hadoop   1048583 5æœˆ   9 09:52 blk_1073741826_1002.meta
-rw-rw-r--. 1 hadoop hadoop 134217728 5æœˆ   9 09:52 blk_1073741827
-rw-rw-r--. 1 hadoop hadoop   1048583 5æœˆ   9 09:52 blk_1073741827_1003.meta
-rw-rw-r--. 1 hadoop hadoop  31376332 5æœˆ   9 09:52 blk_1073741828
-rw-rw-r--. 1 hadoop hadoop    245135 5æœˆ   9 09:52 blk_1073741828_1004.meta

# å°è¯•ä½¿ç”¨ `cat >>` å‘½ä»¤è¿›è¡Œæ–‡ä»¶æ‹¼æ¥ï¼Œæ³¨æ„æ‹¼æ¥éœ€æŒ‰ç…§ `block id` æœ‰æ•ˆåˆ°å¤§çš„é¡ºåºè¿›è¡Œ
[hadoop@hadoop000 subdir0]$ cat blk_1073741825 >> hadoop123
[hadoop@hadoop000 subdir0]$ cat blk_1073741826 >> hadoop123
[hadoop@hadoop000 subdir0]$ cat blk_1073741827 >> hadoop123
[hadoop@hadoop000 subdir0]$ cat blk_1073741828 >> hadoop123
[hadoop@hadoop000 subdir0]$ ll
-rw-rw-r--. 1 hadoop hadoop 134217728 5æœˆ   9 09:51 blk_1073741825
-rw-rw-r--. 1 hadoop hadoop   1048583 5æœˆ   9 09:51 blk_1073741825_1001.meta
-rw-rw-r--. 1 hadoop hadoop 134217728 5æœˆ   9 09:52 blk_1073741826
-rw-rw-r--. 1 hadoop hadoop   1048583 5æœˆ   9 09:52 blk_1073741826_1002.meta
-rw-rw-r--. 1 hadoop hadoop 134217728 5æœˆ   9 09:52 blk_1073741827
-rw-rw-r--. 1 hadoop hadoop   1048583 5æœˆ   9 09:52 blk_1073741827_1003.meta
-rw-rw-r--. 1 hadoop hadoop  31376332 5æœˆ   9 09:52 blk_1073741828
-rw-rw-r--. 1 hadoop hadoop    245135 5æœˆ   9 09:52 blk_1073741828_1004.meta
-rw-rw-r--. 1 hadoop hadoop 434029516 5æœˆ   9 10:06 hadoop123

# æ‹¼æ¥æˆçš„æ–‡ä»¶ hadoop123 ä¸ å­˜å…¥åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿå‰çš„åŸæ–‡ä»¶æ˜¯åŒä¸€ä¸ªæ–‡ä»¶çš„å‰¯æœ¬ï¼Œå¯ä»¥å¯¼å‡ºè‡³æœ¬åœ°å¹¶æ­£å¸¸ä½¿ç”¨
```

$åˆ é™¤æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶$

åˆ é™¤æ–‡ä»¶

``` bash
hdfs dfs -rm æ–‡ä»¶ç›¸å¯¹è·¯å¾„
```

åˆ é™¤ç›®å½•

``` bash
# åˆ é™¤ç©ºç›®å½•ï¼ˆå…¶ä¸‹æ— æ–‡ä»¶æˆ– å­ç›®å½•ï¼‰
hdfs dfs -rmdir ç©ºç›®å½•
# é€’å½’åˆ é™¤æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰å­ç›®å½•
hdfs dfs -rm -r ç›®å½•
hdfs dfs -rmr ç›®å½•
```

æŸ¥çœ‹

``` bash
hdfs dfs -ls /
```

## HDFS å®æˆ˜ï¼šæ–‡ä»¶è¯é¢‘ç»Ÿè®¡

ä½¿ç”¨ HDFS Java API å®ç°HDFS æ–‡ä»¶ç³»ç»Ÿä¸­æ–‡ä»¶çš„è¯é¢‘ç»Ÿè®¡åŠŸèƒ½
è¯é¢‘ç»Ÿè®¡ï¼šwordcount

æ›¿ä»£æ–¹æ³•ï¼šMapReduce/spark

## replication policy

ä¸€èˆ¬ç­–ç•¥

1. æœ¬ rack çš„ä¸€ä¸ªèŠ‚ç‚¹ä¸Š
2. å¦å¤–ä¸€ä¸ª rack çš„ä¸€ä¸ªèŠ‚ç‚¹ä¸Š
3. ä¸ 2 ç›¸åŒçš„ rack çš„æ‹ä¸€ä¸ªèŠ‚ç‚¹ä¸Š

---

1. æœ¬ rack çš„ä¸€ä¸ªèŠ‚ç‚¹ä¸Š
2. æœ¬ rack çš„å¦å¤–ä¸€ä¸ªèŠ‚ç‚¹ä¸Š
3. ä¸åŒrack çš„ä¸€ä¸ªèŠ‚ç‚¹ä¸Š

![Xnip2019-05-11_23-19-56](/assets/Xnip2019-05-11_23-19-56.png)
![Xnip2019-05-11_23-26-09](/assets/Xnip2019-05-11_23-26-09.png)

## HDFS å…ƒæ•°æ®ç®¡ç†

$å…ƒæ•°æ®$

HDFS çš„ç›®å½•ç»“æ„ä»¥åŠæ¯ä¸ªæ–‡ä»¶çš„ block ä¿¡æ¯ï¼ˆblock IDï¼Œreplication sumï¼ŒDataNode locationï¼‰

å­˜å‚¨ä½ç½®ï¼š`~/tmp/dfs/name/current/`
å…ƒæ•°æ®å­˜å‚¨åœ¨æ–‡ä»¶ä¸­

$checkpoint æœºåˆ¶$
![Xnip2019-05-11_23-49-13](/assets/Xnip2019-05-11_23-49-13.png)

$safemode$

safemodeå¤„äº on çŠ¶æ€æ—¶ï¼ŒHDFS æ— æ³•æ‰§è¡Œè¯»å†™æ“ä½œï¼Œè¿›è¿‡ 30 ç§’æ—¶é—´ï¼Œä¼šè‡ªåŠ¨è½¬ä¸º off çŠ¶æ€ã€‚

On startup, the NameNode enters a special state called Safemode. `Replication of data blocks does not occur` when the NameNode is in the Safemode state. The NameNode receives `Heartbeat` and `Blockreport` messages from the DataNodes. A Blockreport contains the list of data blocks that a DataNode is hosting. Each block has a `specified minimum number of replicas`. `A block is considered safely replicated when the minimum number of replicas of that data block has checked in with the NameNode`. After a configurable percentage of safely replicated data blocks checks in with the NameNode (**`plus an additional 30 seconds`**), the NameNode exits the Safemode state. It then determines the list of data blocks (if any) that still have fewer than the specified number of replicas. The NameNode then replicates these blocks to other DataNodes.

åœ¨å¯åŠ¨æ—¶ï¼ŒNameNodeè¿›å…¥ä¸€ä¸ªåä¸ºSafemodeçš„ç‰¹æ®ŠçŠ¶æ€ã€‚å½“NameNodeå¤„äºSafemodeçŠ¶æ€æ—¶ï¼Œä¸ä¼šå‘ç”Ÿæ•°æ®å—çš„å¤åˆ¶ã€‚NameNodeä»DataNodeæ¥æ”¶Heartbeatå’ŒBlockreportæ¶ˆæ¯ã€‚BlockreportåŒ…å«DataNodeæ‰˜ç®¡çš„æ•°æ®å—åˆ—è¡¨ã€‚æ¯ä¸ªå—éƒ½æœ‰æŒ‡å®šçš„æœ€å°å‰¯æœ¬æ•°ã€‚å½“ä½¿ç”¨NameNodeæ£€å…¥è¯¥æ•°æ®å—çš„æœ€å°å‰¯æœ¬æ•°æ—¶ï¼Œä¼šè®¤ä¸ºè¯¥å—æ˜¯å®‰å…¨å¤åˆ¶çš„ã€‚åœ¨å¯é…ç½®ç™¾åˆ†æ¯”çš„å®‰å…¨å¤åˆ¶æ•°æ®å—ä½¿ç”¨NameNodeæ£€å…¥ï¼ˆå†åŠ ä¸Š30ç§’ï¼‰åï¼ŒNameNodeé€€å‡ºSafemodeçŠ¶æ€ã€‚ç„¶åï¼Œå®ƒç¡®å®šä»ç„¶å…·æœ‰å°‘äºæŒ‡å®šæ•°é‡çš„å‰¯æœ¬çš„æ•°æ®å—åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚ç„¶åï¼ŒNameNodeå°†è¿™äº›å—å¤åˆ¶åˆ°å…¶ä»–DataNodeã€‚

## mapreduce

mapreduceä¼˜ç‚¹ï¼šæµ·é‡æ•°æ®ç¦»çº¿å¤„ç†ã€æ˜“å¼€å‘ã€æ˜“è¿è¡Œ
ç¼ºç‚¹ï¼šæ— æ³•é€‚ç”¨äºå®æ—¶æµå¼è®¡ç®—

![Xnip2019-05-12_16-42-56](/assets/Xnip2019-05-12_16-42-56.png)

åœ¨ç¼–ç¨‹æ¨¡å‹ä¸­éœ€è¦åšçš„å·¥ä½œæ˜¯ `spliting -> mapping` ä»¥åŠ `shuffling -> reducing` çš„æ“ä½œã€‚

![Xnip2019-05-12_17-00-15](/assets/Xnip2019-05-12_17-00-15.png)

![mapreduce-job-execution-flow-1-1024x492-1](/assets/mapreduce-job-execution-flow-1-1024x492-1.jpg)

### Combiner æ“ä½œ

ä¼˜ç‚¹ï¼šå‡å°‘ IOï¼Œæå‡ä½œä¸šæ‰§è¡Œæ•ˆèƒ½
ç¼ºç‚¹ï¼šé€‚ç”¨äºå’Œç§¯æ“ä½œï¼Œå¯¹äºé™¤æ“ä½œï¼Œä¾‹å¦‚ï¼Œæ±‚å¹³å‡æ•°ç­‰ï¼Œä¼šä½¿å¾—ç»“æœå¤±æ•ˆã€‚

![Xnip2019-05-13_17-15-06](/assets/Xnip2019-05-13_17-15-06.png)

## YARN

### äº§ç”ŸèƒŒæ™¯

* MapReduce1.x å­˜åœ¨é—®é¢˜ master/salve - JobTrackerã€TaskTracker
  * JobTracker: å•ç‚¹ï¼Œå‹åŠ›å¤§
  * åªèƒ½å¤Ÿæ”¯æŒ MapReduce ä½œä¸šï¼Œä¸èƒ½æ”¯æŒ spark ä½œä¸š
* èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ï¼Œè¿ç»´æˆæœ¬é«˜


![Xnip2019-05-13_23-29-19](/assets/Xnip2019-05-13_23-29-19.png)
![Xnip2019-05-13_23-32-45](/assets/Xnip2019-05-13_23-32-45.png)

The ResourceManager has two main components: **Scheduler** and **ApplicationsManager**.

`The Scheduler is responsible for allocating resources to the various running applications subject to familiar constraints of capacities, queues etc`. The Scheduler is pure scheduler in the sense that it performs no monitoring or tracking of status for the application. Also, it offers no guarantees about restarting failed tasks either due to application failure or hardware failures. The Scheduler performs its scheduling function based the resource requirements of the applications; it does so based on the abstract notion of a resource Container which incorporates elements such as memory, cpu, disk, network etc. In the first version, only memory is supported.

The Scheduler has a pluggable policy plug-in, which is responsible for partitioning the cluster resources among the various queues, applications etc. The current Map-Reduce schedulers such as the CapacityScheduler and the FairScheduler would be some examples of the plug-in.

The CapacityScheduler supports hierarchical queues to allow for more predictable sharing of cluster resources

The ApplicationsManager is responsible for accepting job-submissions, **negotiating the first container for executing the application specific ApplicationMaster** and provides the service for restarting the ApplicationMaster container on failure.

The NodeManager is the per-machine framework agent who is responsible for containers, monitoring their resource usage (cpu, memory, disk, network) and reporting the same to the ResourceManager/Scheduler.

The per-application ApplicationMaster has the responsibility of negotiating appropriate resource containers from the Scheduler, tracking their status and monitoring for progress.

---

ResourceManageræœ‰ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šSchedulerå’ŒApplicationsManagerã€‚

Scheduler è´Ÿè´£æ ¹æ®ç†Ÿæ‚‰çš„å®¹é‡ï¼Œé˜Ÿåˆ—ç­‰çº¦æŸå°†èµ„æºåˆ†é…ç»™å„ç§æ­£åœ¨è¿è¡Œçš„åº”ç”¨ç¨‹åºã€‚Scheduleræ˜¯çº¯è°ƒåº¦ç¨‹åºï¼Œå› ä¸ºå®ƒä¸æ‰§è¡Œåº”ç”¨ç¨‹åºçŠ¶æ€çš„ç›‘è§†æˆ–è·Ÿè¸ªã€‚æ­¤å¤–ï¼Œç”±äºåº”ç”¨ç¨‹åºæ•…éšœæˆ–ç¡¬ä»¶æ•…éšœï¼Œå®ƒæ— æ³•ä¿è¯é‡æ–°å¯åŠ¨å¤±è´¥çš„ä»»åŠ¡ã€‚Scheduleræ ¹æ®åº”ç”¨ç¨‹åºçš„èµ„æºéœ€æ±‚æ‰§è¡Œå…¶è°ƒåº¦åŠŸèƒ½; å®ƒåŸºäºèµ„æºContainerçš„æŠ½è±¡æ¦‚å¿µè¿™æ ·åšï¼Œå®ƒåŒ…å«å†…å­˜ï¼Œcpuï¼Œç£ç›˜ï¼Œç½‘ç»œç­‰å…ƒç´ ã€‚åœ¨ç¬¬ä¸€ä¸ªç‰ˆæœ¬ä¸­ï¼Œåªæ”¯æŒå†…å­˜ã€‚

Schedulerå…·æœ‰å¯æ’å…¥çš„ç­–ç•¥æ’ä»¶ï¼Œè¯¥æ’ä»¶è´Ÿè´£åœ¨å„ç§é˜Ÿåˆ—ï¼Œåº”ç”¨ç¨‹åºç­‰ä¹‹é—´å¯¹é›†ç¾¤èµ„æºè¿›è¡Œåˆ†åŒºã€‚å½“å‰çš„Map-Reduceè°ƒåº¦ç¨‹åºï¼ˆå¦‚CapacitySchedulerå’ŒFairSchedulerï¼‰å°†æ˜¯æ’ä»¶çš„ä¸€äº›ç¤ºä¾‹ã€‚

CapacityScheduleræ”¯æŒåˆ†å±‚é˜Ÿåˆ—ï¼Œä»¥å…è®¸æ›´å¯é¢„æµ‹çš„ç¾¤é›†èµ„æºå…±äº«

ApplicationsManagerè´Ÿè´£æ¥å—ä½œä¸šæäº¤ï¼Œ**åå•†ç¬¬ä¸€ä¸ªå®¹å™¨ä»¥æ‰§è¡Œç‰¹å®šäºåº”ç”¨ç¨‹åºçš„ApplicationMaster**ï¼Œå¹¶æä¾›åœ¨å¤±è´¥æ—¶é‡æ–°å¯åŠ¨ApplicationMasterå®¹å™¨çš„æœåŠ¡ã€‚

NodeManageræ˜¯æ¯å°æœºå™¨æ¡†æ¶ä»£ç†ï¼Œè´Ÿè´£å®¹å™¨ï¼Œç›‘è§†å…¶èµ„æºä½¿ç”¨æƒ…å†µï¼ˆCPUï¼Œå†…å­˜ï¼Œç£ç›˜ï¼Œç½‘ç»œï¼‰å¹¶å°†å…¶æŠ¥å‘Šç»™ResourceManager / Schedulerã€‚

æ¯ä¸ªåº”ç”¨ç¨‹åºApplicationMasterè´Ÿè´£ä»è°ƒåº¦ç¨‹åºåå•†é€‚å½“çš„èµ„æºå®¹å™¨ï¼Œè·Ÿè¸ªå…¶çŠ¶æ€å¹¶ç›‘è§†è¿›åº¦ã€‚

### æ¦‚è¿°

* Yet Another Resource Negitiator
* é€šç”¨èµ„æºç®¡ç†ç³»ç»Ÿ
* ä¸ºä¸Šå±‚åº”ç”¨ï¼ˆmapreduce/spark/hbaseï¼‰æä¾›ç»Ÿä¸€çš„èµ„æºç®¡ç†å’Œè°ƒåº¦

**Apache Hadoop NextGen MapReduce (YARN)**
MapReduce has undergone a complete overhaul in hadoop-0.23 and we now have, what we call, MapReduce 2.0 (MRv2) or YARN.

The fundamental idea of MRv2 is to split up the two major functionalities of the JobTracker, resource management and job scheduling/monitoring, into separate daemons. The idea is to have a global `ResourceManager (RM)` and per-application `ApplicationMaster` (AM). An application is either a single job in the classical sense of Map-Reduce jobs or a DAG of jobs.

The ResourceManager and per-node slave, the NodeManager (NM), form the data-computation framework. **_The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system_**.

The per-application ApplicationMaster is, in effect, **_a framework specific library and is tasked with negotiating resources from the ResourceManager and working with the NodeManager(s) to execute and monitor the tasks_**.

![Xnip2019-05-13_23-48-09](/assets/Xnip2019-05-13_23-48-09.png)

### yarn æ¶æ„

Clientã€1-ResourceManagerã€n-NodeManagerã€n-ApplicationMaster

master/slave: RM/NM

* Client: 
  * å‘ ResourceManager æäº¤ taskï¼Œæ€æ­» task
* ResourceManager: 
  * å¤„ç†æ¥è‡ª Client çš„è¯·æ±‚ï¼Œå¯åŠ¨ç›‘æ§ ApplicationMaster å’Œ NodeManager
  * å¯¹ç³»ç»Ÿä¸­å…¨éƒ¨ application çš„èµ„æºä½¿ç”¨å…·æœ‰æœ€ç»ˆçš„è£å†³æƒåˆ©
  * ä¸”é›†ç¾¤ä¸­ç»Ÿä¸€æ—¶åˆ»å¯¹å¤–æä¾›æœåŠ¡çš„åªæœ‰ä¸€ä¸ªï¼ˆå¤‡ç”¨é™¤å¤–ï¼‰
* ApplicationMaster: 
  * å‘ ResourceManager ç”³è¯·èµ„æºç”¨äºåœ¨ NodeManager ä¸Šæ‰§è¡Œ task
  * ä¸ºæ¯ä¸ª task å‘ ResourceManager ç”³è¯·èµ„æºï¼ˆcontainerï¼‰
  * æ¯ä¸ª application å¯¹åº”ä¸€ä¸ª ApplicationMaster
  * æ•°æ®åˆ‡åˆ†
  * ä¸ NodeManager é€šä¿¡
* NodeManager: 
  * æ¥æ”¶æ¥è‡ª ResourceManager çš„è¯·æ±‚ï¼Œå¹¶æ‰§è¡Œ Task
  * å¤„ç†æ¥è‡ª ApplicationMaster çš„å‘½ä»¤
  * å‘ ResourceManager å‘é€å¿ƒè·³ä¿¡æ¯ã€ä»»åŠ¡æ‰§è¡Œæƒ…å†µã€node status
* container: ä»»åŠ¡çš„è¿è¡ŒæŠ½è±¡
  * memory/cpu
  * task åœ¨ container ä¸­è¿è¡Œ
  * **å¯ä»¥è¿è¡Œ ApplicationMaster**ï¼Œä¹Ÿå¯ä»¥è¿è¡Œå…·ä½“çš„ MapReduce æˆ–è€… task

yarn çš„æ‰§è¡Œæµç¨‹

![Xnip2019-05-14_12-10-47](/assets/Xnip2019-05-14_12-10-47.png)

### yarn æ‰§è¡Œå¼‚å¸¸å¤„ç†

$å¼‚å¸¸ï¼š$
![Xnip2019-05-14_20-12-22](/assets/Xnip2019-05-14_20-12-22.png)

$è§£å†³æ–¹æ¡ˆï¼š$
![Xnip2019-05-14_20-13-01](/assets/Xnip2019-05-14_20-13-01.png)

$åŸå› $
![Xnip2019-05-14_20-13-13](/assets/Xnip2019-05-14_20-13-13.png)
å†å…³é—­é‡å¯ HDFS å’Œ YARN æœåŠ¡ã€‚

### è‡ªå®šä¹‰ MR task æäº¤åˆ° YARN ä¸Šçš„æ­¥éª¤

1. æ‰“åŒ…æˆ jar åŒ…ï¼Œmvn clean package -DskipTests
2. æŠŠä¸Šé¢çš„ jar æ–‡ä»¶ä¼ å…¥æœåŠ¡å™¨ä»»æ„ä½ç½®ï¼Œå¹¶ä¸”æŠŠè¾“å…¥æ–‡ä»¶ä¼ å…¥æœåŠ¡å™¨ HDFS æŒ‡å®šä½ç½®
3. æ‰§è¡Œä½œä¸š hadoop jar xxx å®Œæ•´ç±»å å‚æ•°0 å‚æ•°1
4. åœ¨ YARN UIï¼ˆ8088ï¼‰ä»¥åŠè¾“å‡ºç›®å½•æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶

## ç”µå•†é¡¹ç›®å®æˆ˜

$æ­¥éª¤$

* ç”¨æˆ·æ—¥å¿—åˆ†æ
* ç”µå•†å¸¸ç”¨æœ¯è¯­
* é¡¹ç›®éœ€æ±‚
* æ•°æ®å¤„ç†æµç¨‹åŠæŠ€æœ¯æ¶æ„
* éœ€æ±‚å®ç°
* æäº¤åˆ°æœåŠ¡å™¨æ‰§è¡Œ
* æ‰©å±•

### ç”¨æˆ·æ—¥å¿—

æ¯æ¬¡çš„è®¿é—®è¡Œä¸ºï¼Œç‚¹å‡»ï¼Œæœç´¢ã€è¡Œä¸ºè½¨è¿¹ç­‰æ“ä½œï¼Œ
å†å²è¡Œä¸ºæ•°æ® <-- å†å²è®¢å•æ•°æ®

---> æ¨è

$é‡‡é›†ç”¨æˆ·æ—¥å¿—çš„ç›®çš„$

1. æ¨è
2. é¡µé¢å¸ƒå±€å®‰æ’

$ç”¨æˆ·è¡Œä¸ºæ—¥å¿—çš„ç”Ÿæˆæ¸ é“$

1. NGINX
2. å±€éƒ¨ ajax è¯·æ±‚

$åŸå§‹æ—¥å¿—å­—æ®µè¯´æ˜$

* ç¬¬äºŒä¸ªå­—æ®µï¼šURL
* ç¬¬åå››å­—æ®µï¼šIP
* ç¬¬åå…«å­—æ®µï¼šæ—¥å¿—äº§ç”Ÿæ—¶é—´

å­—æ®µè§£æï¼š
    IP -> åœ°å¸‚ï¼šå›½å®¶ã€çœå¸‚ã€åœ°åŒº
    url -> é¡µé¢ id

$ç”¨æˆ·è¡Œä¸ºæ—¥å¿—åˆ†æçš„æ„ä¹‰$

å¼•æµ

$é¡¹ç›®éœ€æ±‚$

* ç»Ÿè®¡é¡µé¢æµè§ˆé‡ PageView
* ç»Ÿè®¡å„çœä»½çš„æµè§ˆé‡
* ç»Ÿè®¡é¡µé¢çš„è®¿é—®é‡ Page ID

$æ•°æ®å¤„ç†æµç¨‹å’ŒæŠ€æœ¯æ¶æ„$

å¯¹æ—¥å¿—çš„å¤„ç†æ–¹å¼é€‰æ‹©

* MapReduce
* hive

![Xnip2019-05-16_21-24-33](/assets/Xnip2019-05-16_21-24-33.png)

### æœåŠ¡å™¨ä¸Šè¿è¡Œä½œä¸š

![Xnip2019-05-17_23-49-48](/assets/Xnip2019-05-17_23-49-48.png)

å¤§æ•°æ®å¤„ç†å®Œçš„æ•°æ®ä¸€èˆ¬å­˜æ”¾åœ¨ HDFS ä¸Šï¼Œè‡³æ­¤ä¸ºæ­¢ï¼Œæˆ–è€…ä½œè¿›ä¸€æ­¥å­˜åœ¨æ•°æ®åº“ï¼ˆsqoopï¼‰ï¼Œå¹¶è¿›è¡Œå‰ç«¯å±•ç¤º

### æ‰©å±•

* ä½¿ç”¨ userAgentå·¥å…·ç±»è§£æè®¿é—®ç»ˆç«¯ä¿¡æ¯ï¼Œå¹¶ç»Ÿè®¡
* å¼•æµç»Ÿè®¡
* åŒ sessionID è®¡ç®—è®¿é—®æ­¥é•¿ï¼Œä»è€Œç»Ÿè®¡ç”¨æˆ·é»æ€§
* å¼•æµå‰çš„å­—æ®µï¼Œè€—è´¹æµé‡å¤§å°
* etl ä¼˜åŒ–
* çƒ­é›†ç¾¤-å†·é›†ç¾¤-å¯¹è±¡å­˜å‚¨
* etl å­˜å‚¨æ–¹å¼è½¬æ¢ä¸ºåˆ—å¼å­˜å‚¨

6.20~6.21 èŠ‚

## æ•°æ®ä»“åº“ hive

### èƒŒæ™¯

* MapReduce ç¼–ç¨‹çš„ä¸ä¾¿æ€§
* åˆ©ç”¨å…³ç³»å‹æ•°æ®åº“çš„ä¾¿åˆ©
* HDFS ä¸Šçš„æ•°æ®å¹¶æ²¡æœ‰ schema çš„æ¦‚å¿µ

 1. ç”± Facebook å¼€æºï¼Œç”¨æ¥å¤„ç†æµ·é‡ç»“æ„åŒ–æ—¥å¿—çš„ç»Ÿè®¡è®¡ç®—ã€‚
 2. æ„å»ºåœ¨ Hadoop ä¹‹ä¸Šçš„æ•°æ®ä»“åº“
 3. hive æä¾›SQLæŸ¥è¯¢è¯­è¨€ï¼šHQL
 4. åº•å±‚æ”¯æŒå¤šç§ä¸åŒæ‰§è¡Œå¼•æ“ï¼Œï¼ˆMapReduce æ˜¯å…¶ä¸­çš„ä¸€ç§ï¼Œï¼‰

Hive çš„åº•å±‚æ‰§è¡Œå¼•æ“ï¼šMapReduce Tez Spark

Hive ä¼˜ç‚¹ï¼š

* sql ç®€å•æ˜“ä¸Šæ‰‹
* ä¸ºè¶…å¤§æ•°æ®é›†è®¾è®¡çš„è®¡ç®—å’Œæ‰©å±•èƒ½åŠ›
* ç»Ÿä¸€å…ƒæ•°æ®ç®¡ç†ï¼š
  * hive æ•°æ®å­˜å‚¨åœ¨ HDFS ä¸Šï¼Œå…ƒæ•°æ®ï¼ˆè®°å½•æ•°æ®çš„æ•°æ®ï¼‰ä¿¡æ¯å­˜æ”¾åœ¨ MySQL ä¸Š 
  * SQL on Hadoopï¼šHive Spark SQL Impala (äº’é€š)

hive -> SQL query

### ä½“ç³»æ¶æ„

    client: shell thrift/jdbc(server/jdbs) webui(hue/zeppelin)
    metastore: å­˜æ”¾åœ¨ MySQL ä¸Š
               database -> name owner location
               table -> name owner location column type/name
    Diver: sql parser -> query optimizer -> phsyical plan -> serdes udfs/execution
    Mapreduce
    HDFS
![Xnip2019-05-22_16-13-37](/assets/Xnip2019-05-22_16-13-37.png)

### Hive éƒ¨ç½²æ¶æ„

$æµ‹è¯•ç¯å¢ƒ$

![Xnip2019-05-22_16-28-37](/assets/Xnip2019-05-22_16-28-37.png)

$ç”Ÿäº§ç¯å¢ƒ$

![Xnip2019-05-22_16-33-28](/assets/Xnip2019-05-22_16-33-28.png)

### hive å’Œ RDBMS çš„åŒºåˆ«

* SQL è¯­è¨€å½¢å¼ç›¸ä¼¼ï¼Œå‰è€…å¤šç”¨åœ¨ç¦»çº¿å¤„ç†ä¸Šï¼Œå¤„ç†é€Ÿåº¦æ¯”è¾ƒæ…¢ï¼Œåè€…å¤„ç†é€Ÿåº¦è¾ƒå¿«
* æ”¯æŒé›†ç¾¤å¤„ç†ï¼Œä½†æ˜¯ hive çš„é›†ç¾¤å¤„ç†éƒ¨ç½²åœ¨å»‰ä»·çš„åŠå…¶ä¸Šï¼Œå¯ä»¥å¤„ç†æå¤§æ•°æ®é›†ï¼ŒRDBMS éƒ¨ç½²åœ¨é«˜æ˜‚çš„æœºå™¨ä¸Šï¼Œå¤„ç†æ•°æ®é›†ç¾¤è¾ƒå°
* å¤šæ”¯æŒåˆ†å¸ƒå¼å¤„ç†

### hive å®‰è£…éƒ¨ç½²

7.9 èŠ‚

1. ä¸‹è½½
2. è§£å‹ï¼štar -zxvh  -C ~/app/
3. å‘ profile ä¸­æ·»åŠ ç¯å¢ƒå˜é‡
4. é…ç½® hive-env.shï¼Œæ·»åŠ  HADOOP_HOME
5. é…ç½® hive-site.xml
6. æ‹·è´ MySQL driver 5.1.27 åˆ°$HIVE_HOME/lib
7. å®‰è£… MySQL æ•°æ®åº“ï¼ˆå®‰è£…æœªæˆåŠŸï¼Œå»¶åï¼‰
   1. https://www.cnblogs.com/julyme/p/5969626.html 

hive-site.xml æ–‡ä»¶

``` xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://hadoop000:3306/hadoop_hive?createDatabaseIfNotExist=true</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>root</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>root</value>
</property>
</configuration>
```

``` bash
# ç™»å…¥ MySQL
mysql -uroot -proot

# æ˜¾ç¤ºæ•°æ®åº“ï¼Œ
show databases;

# å¯åŠ¨ hive
# ä½¿ç”¨ `hive` æˆ–è€… `beeline`
cd $HIVE_HOME/bin/
hive
beeline

# åˆ›å»º test-db æ•°æ®åº“
hive> create database test_db;

# æ˜¾ç¤ºæ•°æ®åº“ï¼Œæ­¤æ—¶æ–°å¢äº†åä¸º `hadoop_hive` çš„æ•°æ®
# è¯¥æ•°æ®åº“ä¸­å­˜æ”¾çš„æ˜¯ **å…ƒæ•°æ®ä¿¡æ¯**
show databases;

# ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹æ•°æ®åº“ hadoop_hive ä¸­çš„ table
use hadoop_hive;
show tables;

# å…¶ä¸­æœ‰ä¸€ä¸ª åä¸º DBS çš„è¡¨ï¼Œä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å¯ä»¥æ˜¾ç¤ºå…¶å†…å®¹
# é‡Œé¢æœ‰ä¸€ä¸ª åä¸º default çš„æ•°æ®åº“å’Œä¸€ä¸ªä¹‹å‰æ–°å»ºçš„ åä¸º test_db æ•°æ®åº“
select * from DBS \G;

# åœ¨æ•°æ®åº“ test_db ä¸‹åˆ›å»ºåä¸º helloworld çš„è¡¨
hive> use test_db;
hive> show tables;
hive> create table helloworld(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
hive> show tables;
# ä»¥æ–‡æœ¬çš„å½¢å¼å‘è¡¨ä¸­æ·»åŠ è®°å½•
hive> load data local inpath '/home/hadoop/data/helloworld.txt' into table helloworld;

# æŸ¥çœ‹è¡¨ helloworld ä¸­çš„æ•°æ®
hive> select * from helloworld;

# ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤ç»Ÿè®¡ è¡¨ä¸­çš„è®°å½•æ•°ï¼Œä¼šäº§ç”Ÿ MapReduce taskï¼Œå¯ä»¥åœ¨ yarn ç½‘å€ä¸ŠæŸ¥çœ‹
hive> select count(1) from helloworld;
```

### Hive DDL

DDLï¼šHive Data Definition Language

> createã€deleteã€alter...

Hiveæ•°æ®æŠ½è±¡/ç»“æ„

* database     HDFSä¸€ä¸ªç›®å½•
  * table    HDFSä¸€ä¸ªç›®å½•
    * data  æ–‡ä»¶
      * partition åˆ†åŒºè¡¨  HDFSä¸€ä¸ªç›®å½•
        * data  æ–‡ä»¶
        * bucket  åˆ†æ¡¶   HDFSä¸€ä¸ªæ–‡ä»¶

$åˆ›å»ºæ•°æ®åº“$

``` sql
CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name
  [COMMENT database_comment]
  [LOCATION hdfs_path]
  [WITH DBPROPERTIES (property_name=property_value, ...)];

CREATE DATABASE IF NOT EXISTS hive;

CREATE DATABASE IF NOT EXISTS hive2 LOCATION '/test/location';

CREATE DATABASE IF NOT EXISTS hive3 
WITH DBPROPERTIES('creator'='pk');

-- æŸ¥çœ‹ creator ä¿¡æ¯
hive> desc database hive;
hive> desc database extended hive;

-- æŸ¥çœ‹å½“å‰åœ¨å“ªä¸ª database ä¸Šè¿›è¡Œæ“ä½œï¼Œå‰å¥æ˜¾ç¤ºå±æ€§å€¼ï¼Œåå¥èµ‹å€¼ç»™ç›¸å…³å±æ€§èµ‹å€¼
hive> set hive.cli.print.current.db
hive> set hive.cli.print.current.db = true

-- æ¸…å±
hive> !clear

-- åˆ é™¤ databaseï¼Œåˆ é™¤éç©ºè¡¨éœ€è¦æ·»åŠ  `CASCADE`
hive> drop database database_name
hive> drop database database_name CASCADE
hive> show database like 'hive*'

```

$åˆ›å»ºè¡¨$

``` sql
-- ä½¿ç”¨ default database
hive> use default;
hive> CREATE TABLE emp(
  empno int,
  ename string,
  job string,
  mgr int,
  hiredate string,
  sal double,
  comm double,
  deptno int
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY ' ';

-- æŸ¥çœ‹è¡¨ç»“æ„
hive> desc emp;

-- æŸ¥çœ‹è¡¨çš„å­˜å‚¨ä¿¡æ¯
hive> desc extended emp;
hive> desc formatted emp;

-- å°†æ–‡ä»¶ä¸­çš„æ•°æ®æ³¨å…¥è¡¨ emp ä¸­
hive> LOAD DATA LOCAL INPATH '/home/hadoop/data/emp.txt' OVERWRITE INTO TABLE emp;

-- æŸ¥çœ‹è¡¨ emp çš„å†…å®¹
hive> select * from emp;

-- æŸ¥çœ‹ warehouse ä¸­çš„ emp è¡¨
$ hadoop fs -ls /user/hive/warehouse/emp
-- è¾“å‡ºä¸ºï¼š/user/hive/warehouse/emp/emp.txt
-- å³å°±æ˜¯è¡¨ emp åä¸º emp.txt

-- ä¿®æ”¹è¡¨è¡¨åç§°
ALTER TABLE table_name RENAME TO new_table_name;
```






### hive æ•°æ®æŠ½è±¡ã€ç»“æ„

---

HDFSä¸Šçš„æ–‡ä»¶å¹¶æ²¡æœ‰schemaçš„æ¦‚å¿µ

* schema

Hiveåº•å±‚æ‰§è¡Œå¼•æ“æ”¯æŒï¼šMR/Tez/Spark

ç»Ÿä¸€å…ƒæ•°æ®ç®¡ç†ï¼š

* Hiveæ•°æ®æ˜¯å­˜æ”¾åœ¨HDFS
* å…ƒæ•°æ®ä¿¡æ¯(è®°å½•æ•°æ®çš„æ•°æ®)æ˜¯å­˜æ”¾åœ¨MySQLä¸­
* SQL on Hadoopï¼š Hiveã€Spark SQLã€impala....

Hiveä½“ç³»æ¶æ„
	clientï¼šshellã€thrift/jdbc(server/jdbc)ã€WebUI(HUE/Zeppelin)
	metastoreï¼š==> MySQL
		databaseï¼šnameã€locationã€owner....
		tableï¼šnameã€locationã€ownerã€column name/type ....




/user/hive/warehouseæ˜¯Hiveé»˜è®¤çš„å­˜å‚¨åœ¨HDFSä¸Šçš„è·¯å¾„

``` sql

LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]

LOCALï¼šæœ¬åœ°ç³»ç»Ÿï¼Œå¦‚æœæ²¡æœ‰localé‚£ä¹ˆå°±æ˜¯æŒ‡çš„HDFSçš„è·¯å¾„
OVERWRITEï¼šæ˜¯å¦æ•°æ®è¦†ç›–ï¼Œå¦‚æœæ²¡æœ‰é‚£ä¹ˆå°±æ˜¯æ•°æ®è¿½åŠ 

LOAD DATA LOCAL INPATH '/home/hadoop/data/emp.txt' OVERWRITE INTO TABLE emp;

LOAD DATA INPATH 'hdfs://hadoop000:8020/data/emp.txt' INTO TABLE emp;

INSERT OVERWRITE LOCAL DIRECTORY '/tmp/hive/'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
select empno,ename,sal,deptno from emp;
```

èšåˆï¼š max/min/sum/avg

åˆ†ç»„å‡½æ•°ï¼š group by

``` sql
-- æ±‚æ¯ä¸ªéƒ¨é—¨çš„å¹³å‡å·¥èµ„
-- å‡ºç°åœ¨selectä¸­çš„å­—æ®µï¼Œå¦‚æœæ²¡æœ‰å‡ºç°åœ¨èšåˆå‡½æ•°é‡Œï¼Œé‚£ä¹ˆä¸€å®šè¦å®ç°åœ¨group byé‡Œ
select deptno, avg(sal) from emp group by deptno;

-- æ±‚æ¯ä¸ªéƒ¨é—¨ã€å·¥ä½œå²—ä½çš„å¹³å‡å·¥èµ„
select deptno,job avg(sal) from emp group by deptno,job;


-- æ±‚æ¯ä¸ªéƒ¨é—¨çš„å¹³å‡å·¥èµ„å¤§äº2000çš„éƒ¨é—¨
select deptno, avg(sal) avg_sal from emp group by deptno where avg_sal>2000;

-- å¯¹äºåˆ†ç»„å‡½æ•°è¿‡æ»¤è¦ä½¿ç”¨having
select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal>2000;
```

join ï¼š å¤šè¡¨

``` sql
emp
dept

CREATE TABLE dept(
deptno int,
dname string,
loc string
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

LOAD DATA LOCAL INPATH '/home/hadoop/data/dept.txt' OVERWRITE INTO TABLE dept;

explain EXTENDED
select e.empno,e.ename,e.sal,e.deptno,d.dname from emp e join dept d
on e.deptno=d.deptno;
```

---
