# 大数据基础

## 大数据带来的技术驱动

* 技术驱动：数据量大
  * 存储：文件存储 ==> 分布式存储
  * 计算：单机 ==> 分布式计算 MapReduce
  * 网络：万兆带宽
  * DB：RDBMS ==> NoSQL(HBase/Redis...)
* 商业驱动：价值驱动

## 大数据现存的模式

* 拥数据，没有大数据思维
* 没有大数据，有大数据思维
* 既有大数据，又有大数据思维

## 大数据的技术概念

* 单机：CPU memory disk
* 分布式并行计算/处理

$过程$

* 数据采集（精细化筛选）：flume Sqoop
* 数据存储：Hadoop
* 数据处理/分析/挖掘：Hadoop、Spark、Flink...
* 可视化：

## 大数据技术架构

* 对现有数据库管理技术的挑战：MySQL->NoSQL
* 经典的数据库技术并没有考虑到数据的多类型
* 实时性的技术挑战
* 网络架构、数据中心、运维的挑战
* 隐私防泄漏
* 数据源复杂多样的挑战

## 如何对大数据进行存储和分析

|系统瓶颈|Google 大数据技术|
|:---:|:---:|
|存储容量|MapReduce|
|读写速度|bigtable|
|计算效率|GFS|

## 大数据典型应用

Count、Sum、AVG -> group by/join -> 窗口分析函数 -> 异常、欺诈检测 -> 人工智能

报表 -> 用户细分 -> 指标监控 -> 指标预警

## 初识Hadoop

### Hadoop 概述

logo 来源于动物，Hadoop 之父是 Doug Cutting。
Hadoop 是 Apache 社区的顶级项目：xxxx.apache.org
hadoop hive hbase spark flink storm

The Apache hadoop project develops open-source software for `reliable, scalable, distributed computing`.

The Apache Hadoop software libary is a framework that allows for the `distributed processing` of large data sets across clusters of computers using simple programming models.

Hadoop 提供`分布式存储 + 分布式计算`，是一个分布式的系统基础架构。

分布式存储：一个文件被拆分成很多块，并且以副本的方式存储在各个节点中。
分布式系统架构：用户可以在不了解分布式底层细节的前提下使用。

### Hadoop 核心组件

* **Hadoop Common**: The common utilities that support the other Hadoop modules.
* **Hadoop Distributed File System(HDFS-分布式文件系统)**: A distributed file system that provides high-throughput access to application data.
* **Hapoop Yarn(分布式资源调度框架)**: A framework for `job scheduling` and `cluster resource management`
* **Hadoop MapReduce(分布式计算框架)**: A Yarn-based system for parallel processing of large data sets
* **Hadoop Ozone(对象存储)**: A object store for Hadoop

#### HDFS-分布式文件系统

* 源于 Google 的 gfs 论文，论文发表与2003年10月
* hdfs 是 gfs 的克隆版
* HDFS 特点：扩展性 & 容错性 & 海量数据存储
* 将文件切分成指定大小(block-size)的`数据块(block)`，并以`多副本(multi-standy)`的方式存储在机器上面
* 数据切分、多副本、容错等操作对用户是透明的(用户无需知晓)

$概述$

The hdfs is a distributed file system designed to run on commodity hardware. It has many simikarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX(可移植性操作系统接口) requirements to enable streaming access to file system data.

1. distributed
2. commodity hardware or low-cost hardware
3. fault-tolerant
4. high throughput access
5. large data sets

普通文件系统 vs 分布式文件系统

    单机
    分布式文件系统能够横跨 N 个机器

$HDFS前提和设计目标$

* **Hardware Failure**: Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system's data. The fact that there are a huge number of components and that each component has a **`non-trivial probability of failure`** means that some component of HDFS is always non-functional(始终不起作用). Therefore, **`detection of faults and quick, automatic recovery from them`** is a core architectural goal of HDFS.

* **Streaming Data Access**: Applications that run on HDFS need **`streaming access`** to their data sets. They are not general purpose applications that typically run on general purpose file systems. HDFS is designed more for **`batch processing`** rather than **`interactive use`** by users. The emphasis is on **`high throughput of data access`** rather than **`low latency of data access`**. POSIX(可移植性操作系统接口) imposes many hard requirements that are not needed for applications that are targeted for HDFS. POSIX semantics in a few areas has been traded to increase data throughput rates.

* **Large Data Sets**: Application that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned(调优的) to support large files. It should provides **`high aggregate data bandwidth(高聚合数据带宽)`** and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.

* **Simple Coherency Model**: 简化的数据一致性问题，并实现了高吞吐量数据访问。

* **Moving Computation is Cheaper than Moving Data**: A computation requested by an application is much more efficient if **`it is executed near the data it operates on`**. This is especially true when the size of the data set is huge. This minizes network congestion and increase the overall throughput of system. The assumption is that it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. `HDFS provides interfaces for application to move themselves closer to where the data is located`.

* **Portability(可移植性、便携性) Across Heterogeneous Hardware and Software Platforms**: HDFS has been designed to be portable from one platform to another. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.

#### 分布式计算框架-MapReduce

* 源于 Google 的 MapReduce 论文，论文发表于2004年12月
* MapReduce 是 Google MapReduce 的克隆版
* MapReduce 特点：扩展性 & 容错性 & 海量数据离线处理
* word count process: input -> splitting -> mapping -> shuffling -> reducing -> final result

#### 资源调度系统 YARN

* YARN：yet another resource negotiator
* 负责整个`集群资源`的管理和调度
* YARN 特点：扩展性 & 容错性 & `多框架资源`统一调度
* 多框架：script-Pig sql-hive nosql-hbase stream-storm in-memory-spark search-solr flink

### Hadoop 优势

$高可靠性$

* 数据存储：数据库多副本
* 数据计算：重新调度作业计算

$高扩展性$

* 存储、计算资源不够时，可以横向线型扩展机器
* 一个集群中可以包含数以千计的节点
* 存储在廉价的机器上(去 IOE)，降低成本
* 成熟的生态圈

### Hadoop 发展史

诞生于2006年，nutch-gfs-mapreduce-bigtable(hbase)-2007百度移动使用-hive-cloudera-cbh-spark-

### Hadoop 生态系统

**狭义的 Hadoop**：是一个适用于大数据分布式存储（HDFS）、分布式计算（MapReduce）和资源调度（YARN）的平台；

**广义的 Hadoop**：指的是 `Hadoop 的生态系统`，它是一个很庞大的概念，Hadoop 是其中最重要最基础的一个部分；生态系统中的一个子系统`只解决某一特定问题域`（甚至可能很窄）不做统一型的一个全能系统，而是`小而精`的`多个小的系统`；

![Xnip2019-04-24_17-33-22](/assets/Xnip2019-04-24_17-33-22.png)

* 开源、社区活跃
* 囊括了大数据处理的方方面面
* 成熟的生态圈

### Hadoop 发行版的选择

* Apache
  * 优点：纯开源，
  * 缺点：不同版本、不同框架之间整合问题多 jar 冲突...
* CDH
  * 优点：http://www.cloudera.com cm（clouder manager）通过页面一键安装各种框架、进行升级操作，
  * 缺点：cm 不开源
* Hortonworks: HDP 企业通过发布自己的数据平台可以直接基于页面框架进行改造
  * 优点：原装 Hadoop、纯开源、支持 tez(分布式dh框架)
  * 缺点：企业级安全不开源，
* MapReduce

## HDFS Architecture

* NameNode and DataNodes
* master/slave architecture

### 架构概述

HDFS has a **`master/slave`** architecture. An HDFS cluster consists of a single NameNode, a master server that **`manages the file system namespace`** and **`regulates access to files by clients`**.

In addition, there are a number of DataNodes, usually one per node in the cluster, which **`manage storage attached to the nodes`** that they are run on.

HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file split into one or more blocks and these blocks are stored in a set of DataNodes(一组 DataNode).

**`The NameNode executes file system namespace operations like opening, closing, and renaming files and directories`**. It also determines the mapping of blocks to DataNodes.

**`The DataNodes are responsible for serving read and write requests from the file system's clients. The DataNodes also perform block creation、deletion、and replication upon instruction from the NameNode`**.

A typical deployment has a dedicated(专有的) machine that runs only **`the NameNode software`**. Each of the other machines in the cluster runs **`one instance of the DataNode software`**. The architecture does not preclude(排斥) running multiple DataNodes on the same machine but in a real deployment that is rarely the case.

### The File System Namespace

HDFS supports a traditional hierarchical(层级的) file organization. A user or an application can create directories and store files inside these directories. **`The file system namespace hierarchy is similar to most other existing file systems`**; one can create and remove files, move a file from one directory to another, or rename a file. HDFS supports **user quotas** and **access permissions**. HDFS does not support hard links or soft links. However, the HDFS architecture does not preclude implementing these features.

**`The NameNode maintains the file system namespace. Any change to the file system namespace or its properties is recorded by the HDFS`**. An application can specify the number of replicas of a file that should be maintained by HDFS. The number of copies of a file is called the **replication factor** of that file. This information is stored by the NameNode.

### Data Replication

HDFS is designed to reliably store very **large files** across machines in a large cluster. It stores each file as a sequence of blocks. **The blocks of file are replicated for fault tolerance**. The block size and replication factor are configurable per file.

All blocks in a file except the last block are the same size, while users can start a new block without filling out the last block to the configured block size after the support for variable length block was added to append and hsync.(同时在将对可变长度 block 的支持添加到“append”和“hsync“功能中后，用户可以以一个新的 block 开始而不必将原有最后一个block 填满至配置的容量规格)

An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once(except for appends and truncates 除过追加和截断) and have strictly(严格地、确实地) one writer at any time.

The NameNode makes all decisions regarding replication of blocks. It periodically receives a **Heartbeat** and a **Blockreport** from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode.

![hdfsdatanodes](/assets/hdfsdatanodes.png)

## linux 操作

linux 用户名：hadoop
密码：123456
HostName: hadoop000

创建文件夹：
    mkdir software 软件安装包存放位置
    mkdir app      软件安装位置
    mkdir data     数据存放位置
    mkdir lib      存放作业 jar
    mkdir shell    存放相关脚本
    mkdir maven_resp 存放 maven依赖包
hadoop 用户转 root 用户：
    ```sudo -i```
反向
    ```su hadoop```
linux 版本：centos7
    ```uname -a```

配置好 IP 及域名映射：
    ```vim /etc/hosts```
    server ip   hostname

## Hadoop 初始安装配置

下载地址：http://archive.cloudera.com/cdh5/cdh/5/
Hadoop 选择hadoop-2.6.0-cdh5.15.1.tar.gz

使用单机版进项框架学习

Hadoop 安装前置要求安装以下软件

* Java 1.8+
* ssh

### 拷贝文件到服务器

``` bash
scp hadoop-2.6.0-cdh5.15.1 hadoop@10.211.55.8:~/software/
```

### 安装：解压缩及环境配置

$解压缩至 `~/app/`$

``` bash
tar -zxvf hadoop-2.6.0-cdh5.15.1 -C ~/app/
```

hadoop 软件包常用目录名单
    bin: hadoop 客户端名单
    etc/hadoop：hadoop相关的配置文件存放目录
    sbin: 启动 hadoop相关进程的脚本

$配置 profile 文件$

``` bash
# 打开 profile 文件
vim ~/etc/profile
# 添加 HADOOP_HOME，并修改 `PATH`，具体如下
export HADOOP_HOME=/usr/hadoop/app/hadoop-2.6.0-cdh5.15.1
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
# 执行下面的命令是 profile 修改生效
source profile
```

$配置 Hadoop 的配置文件hadoop-env.sh$

``` bash
# 打开 hadoop-env.sh 文件```
# 注意在使用 `cd /etc` 命令时，总是进入系统自带 `etc`目录中
vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh
# 查找或添加下句，
# 注意该文件中 `JAVA_HOME` 的值包含 `${JAVA_HOME}` 可能无效 ，建议使用明文路径
export JAVA_HOME=/usr/java/jdk1.8.0_152
```

### 分布式部署

本次测试使用伪分布式部署方式（Pseudo-Distributed Operation）

$配置core-site.xml$

``` bash
# 打开 `core-site.xml` 文件
vim $HADOOP_HOME/etc/hadoop/core-site.xml
# 在 <configuration> 节点中洪添加如下代码
# 注意下面的 `hadoop000` 为 hostname，需在（本地及服务端） `~/etc/hosts` 文件中配置，即加入 `10.211.55.8 hadoop000`，并且应保证端口 9000 不被占用
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://hadoop000:9000</value>
</property>
```

$配置hdfs-site.xml$

``` bash
# 打开 `hdfs-site.xml` 文件
vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml
# 在 <configuration> 节点中洪添加如下代码
# 注意需在 `$HADOOP_HOME` 中创建 `tmp` 目录
# 且 `hadoop.tmp.dir` 对应的 URI 不能包含 `${}` 类型环境变量，否则导致配置无效出错 ，建议使用明文路径
<property>
    <name>dfs.replication</name>
    <value>1</value>
</property>
<property>
    <name>hadoop.tmp.dir</name>
    <value>/usr/hadoop/app/hadoop-2.6.0-cdh5.15.1/tmp</value>
</property>
```

$配置slaves$

``` bash
# 打开 `slaves` 文件，添加 `hadoop000`
vim $HADOOP_HOME/etc/hadoop/slaves
```

### 启动 HDFS

$格式化文件系统$

``` bash
# 在任意目录下执行下面的命令，注意 `hdfs` 命令在 Hadoop 的 bin 目录下
hdfs namenode -format
```

$启动 namenode 和 datanode（Start NameNode daemon and DataNode daemon）$

``` bash
# 注意下面的命令需在 hadoop 的 `sbin` 目录下执行
cd $HADOOP_HOME/bin
start-dfs.sh
```

> 🦋注意：start-dfs.sh = hadoop-daemons.sh start namenode
>                       hadoop-daemons.sh start datanode
>                       hadoop-daemons.sh start secondarynamenode

$验证$

使用 jps 命令进行验证 或者使用 在浏览器地址栏键入 http://10.211.55.8:50070，进行验证。
如使用后一种方法验证不成功，则需关闭防火墙

``` bash
# 检查防火墙状态
firewall-cmd --state
systemctl status firewalld
# 禁用或关闭防火墙
systemctl stop firewalld
```

$停止进程(namenode/datanode/secondarynamenode)$

``` bash
# 注意下面的命令需在 hadoop 的 `sbin` 目录下执行
cd $HADOOP_HOME/bin
stop-dfs.sh
```

> 🦋注意：也可以使用 `hadoop-daemons.sh stop + daemons 名称` 单独执行停止操作

## hadoop 存储机制

$复制文件到分布式文件系统$

``` bash
# 复制文件到分布式文件系统的主目录中
hdfs dfs -put hadoop-2.6.0-cdh5.15.1.tar.gz /
# 查看分布式系统主目录中存有的文件
hdfs dfs -ls /
```

$浏览器及系统中查看文件存储机制$

在浏览器地址栏键入http://10.211.55.8:50070，并点击 `Utilities >> Browse the file system` 查看相关 `block` 信息。

在 $HADOOP_HOME/tmp/dfs/data/current/BP-1788419893-10.211.55.8-1557305523480/current/finalized/subdir0/subdir0 查看存储在分布式文件系统中的分块文件。

``` bash
-rw-rw-r--. 1 hadoop hadoop 134217728 5月   9 09:51 blk_1073741825
-rw-rw-r--. 1 hadoop hadoop   1048583 5月   9 09:51 blk_1073741825_1001.meta
-rw-rw-r--. 1 hadoop hadoop 134217728 5月   9 09:52 blk_1073741826
-rw-rw-r--. 1 hadoop hadoop   1048583 5月   9 09:52 blk_1073741826_1002.meta
-rw-rw-r--. 1 hadoop hadoop 134217728 5月   9 09:52 blk_1073741827
-rw-rw-r--. 1 hadoop hadoop   1048583 5月   9 09:52 blk_1073741827_1003.meta
-rw-rw-r--. 1 hadoop hadoop  31376332 5月   9 09:52 blk_1073741828
-rw-rw-r--. 1 hadoop hadoop    245135 5月   9 09:52 blk_1073741828_1004.meta

# 尝试使用 `cat >>` 命令进行文件拼接，注意拼接需按照 `block id` 有效到大的顺序进行
[hadoop@hadoop000 subdir0]$ cat blk_1073741825 >> hadoop123
[hadoop@hadoop000 subdir0]$ cat blk_1073741826 >> hadoop123
[hadoop@hadoop000 subdir0]$ cat blk_1073741827 >> hadoop123
[hadoop@hadoop000 subdir0]$ cat blk_1073741828 >> hadoop123
[hadoop@hadoop000 subdir0]$ ll
-rw-rw-r--. 1 hadoop hadoop 134217728 5月   9 09:51 blk_1073741825
-rw-rw-r--. 1 hadoop hadoop   1048583 5月   9 09:51 blk_1073741825_1001.meta
-rw-rw-r--. 1 hadoop hadoop 134217728 5月   9 09:52 blk_1073741826
-rw-rw-r--. 1 hadoop hadoop   1048583 5月   9 09:52 blk_1073741826_1002.meta
-rw-rw-r--. 1 hadoop hadoop 134217728 5月   9 09:52 blk_1073741827
-rw-rw-r--. 1 hadoop hadoop   1048583 5月   9 09:52 blk_1073741827_1003.meta
-rw-rw-r--. 1 hadoop hadoop  31376332 5月   9 09:52 blk_1073741828
-rw-rw-r--. 1 hadoop hadoop    245135 5月   9 09:52 blk_1073741828_1004.meta
-rw-rw-r--. 1 hadoop hadoop 434029516 5月   9 10:06 hadoop123

# 拼接成的文件 hadoop123 与 存入分布式文件系统前的原文件是同一个文件的副本，可以导出至本地并正常使用
```

$删除文件系统中的文件$

删除文件

``` bash
hdfs dfs -rm 文件相对路径
```

删除目录

``` bash
# 删除空目录（其下无文件或 子目录）
hdfs dfs -rmdir 空目录
# 递归删除指定目录下所有子目录
hdfs dfs -rm -r 目录
hdfs dfs -rmr 目录
```

查看

``` bash
hdfs dfs -ls /
```