# 大数据基础

## 大数据带来的技术驱动

* 技术驱动：数据量大
  * 存储：文件存储 ==> 分布式存储
  * 计算：单机 ==> 分布式计算 MapReduce
  * 网络：万兆带宽
  * DB：RDBMS ==> NoSQL(HBase/Redis...)
* 商业驱动：价值驱动

## 大数据现存的模式

* 拥数据，没有大数据思维
* 没有大数据，有大数据思维
* 既有大数据，又有大数据思维

## 大数据的技术概念

* 单机：CPU memory disk
* 分布式并行计算/处理

$过程$

* 数据采集（精细化筛选）：flume Sqoop
* 数据存储：Hadoop
* 数据处理/分析/挖掘：Hadoop、Spark、Flink...
* 可视化：

## 大数据技术架构

* 对现有数据库管理技术的挑战：MySQL->NoSQL
* 经典的数据库技术并没有考虑到数据的多类型
* 实时性的技术挑战
* 网络架构、数据中心、运维的挑战
* 隐私防泄漏
* 数据源复杂多样的挑战

## 如何对大数据进行存储和分析

|系统瓶颈|Google 大数据技术|
|:---:|:---:|
|存储容量|MapReduce|
|读写速度|bigtable|
|计算效率|GFS|

## 大数据典型应用

Count、Sum、AVG -> group by/join -> 窗口分析函数 -> 异常、欺诈检测 -> 人工智能

报表 -> 用户细分 -> 指标监控 -> 指标预警

## 初识Hadoop

### Hadoop 概述

logo 来源于动物，Hadoop 之父是 Doug Cutting。
Hadoop 是 Apache 社区的顶级项目：xxxx.apache.org
hadoop hive hbase spark flink storm

The Apache hadoop project develops open-source software for `reliable, scalable, distributed computing`.

The Apache Hadoop software libary is a framework that allows for the `distributed processing` of large data sets across clusters of computers using simple programming models.

Hadoop 提供`分布式存储 + 分布式计算`，是一个分布式的系统基础架构。

分布式存储：一个文件被拆分成很多块，并且以副本的方式存储在各个节点中。
分布式系统架构：用户可以在不了解分布式底层细节的前提下使用。

### Hadoop 核心组件

* **Hadoop Common**: The common utilities that support the other Hadoop modules.
* **Hadoop Distributed File System(HDFS-分布式文件系统)**: A distributed file system that provides high-throughput access to application data.
* **Hapoop Yarn(分布式资源调度框架)**: A framework for `job scheduling` and `cluster resource management`
* **Hadoop MapReduce(分布式计算框架)**: A Yarn-based system for parallel processing of large data sets
* **Hadoop Ozone(对象存储)**: A object store for Hadoop

#### HDFS-分布式文件系统

* 源于 Google 的 gfs 论文，论文发表与2003年10月
* hdfs 是 gfs 的克隆版
* HDFS 特点：扩展性 & 容错性 & 海量数据存储
* 将文件切分成指定大小(block-size)的`数据块(block)`，并以`多副本(multi-standy)`的方式存储在机器上面
* 数据切分、多副本、容错等操作对用户是透明的(用户无需知晓)

$概述$

The hdfs is a distributed file system designed to run on commodity hardware. It has many simikarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX(可移植性操作系统接口) requirements to enable streaming access to file system data.

1. distributed
2. commodity hardware or low-cost hardware
3. fault-tolerant
4. high throughput access
5. large data sets

普通文件系统 vs 分布式文件系统

    单机
    分布式文件系统能够横跨 N 个机器

$HDFS前提和设计目标$

* **Hardware Failure**: Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system's data. The fact that there are a huge number of components and that each component has a **`non-trivial probability of failure`** means that some component of HDFS is always non-functional(始终不起作用). Therefore, **`detection of faults and quick, automatic recovery from them`** is a core architectural goal of HDFS.

* **Streaming Data Access**: Applications that run on HDFS need **`streaming access`** to their data sets. They are not general purpose applications that typically run on general purpose file systems. HDFS is designed more for **`batch processing`** rather than **`interactive use`** by users. The emphasis is on **`high throughput of data access`** rather than **`low latency of data access`**. POSIX(可移植性操作系统接口) imposes many hard requirements that are not needed for applications that are targeted for HDFS. POSIX semantics in a few areas has been traded to increase data throughput rates.

* **Large Data Sets**: Application that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned(调优的) to support large files. It should provides **`high aggregate data bandwidth(高聚合数据带宽)`** and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.

* **Simple Coherency Model**: 简化的数据一致性问题，并实现了高吞吐量数据访问。

* **Moving Computation is Cheaper than Moving Data**: A computation requested by an application is much more efficient if **`it is executed near the data it operates on`**. This is especially true when the size of the data set is huge. This minizes network congestion and increase the overall throughput of system. The assumption is that it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. `HDFS provides interfaces for application to move themselves closer to where the data is located`.

* **Portability(可移植性、便携性) Across Heterogeneous Hardware and Software Platforms**: HDFS has been designed to be portable from one platform to another. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.

#### 分布式计算框架-MapReduce

* 源于 Google 的 MapReduce 论文，论文发表于2004年12月
* MapReduce 是 Google MapReduce 的克隆版
* MapReduce 特点：扩展性 & 容错性 & 海量数据离线处理
* word count process: input -> splitting -> mapping -> shuffling -> reducing -> final result

#### 资源调度系统 YARN

* YARN：yet another resource negotiator
* 负责整个`集群资源`的管理和调度
* YARN 特点：扩展性 & 容错性 & `多框架资源`统一调度
* 多框架：script-Pig sql-hive nosql-hbase stream-storm in-memory-spark search-solr flink

### Hadoop 优势

$高可靠性$

* 数据存储：数据库多副本
* 数据计算：重新调度作业计算

$高扩展性$

* 存储、计算资源不够时，可以横向线型扩展机器
* 一个集群中可以包含数以千计的节点
* 存储在廉价的机器上(去 IOE)，降低成本
* 成熟的生态圈

### Hadoop 发展史

诞生于2006年，nutch-gfs-mapreduce-bigtable(hbase)-2007百度移动使用-hive-cloudera-cbh-spark-

### Hadoop 生态系统

**狭义的 Hadoop**：是一个适用于大数据分布式存储（HDFS）、分布式计算（MapReduce）和资源调度（YARN）的平台；

**广义的 Hadoop**：指的是 `Hadoop 的生态系统`，它是一个很庞大的概念，Hadoop 是其中最重要最基础的一个部分；生态系统中的一个子系统`只解决某一特定问题域`（甚至可能很窄）不做统一型的一个全能系统，而是`小而精`的`多个小的系统`；

![Xnip2019-04-24_17-33-22](/assets/Xnip2019-04-24_17-33-22.png)

* 开源、社区活跃
* 囊括了大数据处理的方方面面
* 成熟的生态圈

### Hadoop 发行版的选择

* Apache
  * 优点：纯开源，
  * 缺点：不同版本、不同框架之间整合问题多 jar 冲突...
* CDH
  * 优点：http://www.cloudera.com cm（clouder manager）通过页面一键安装各种框架、进行升级操作，
  * 缺点：cm 不开源
* Hortonworks: HDP 企业通过发布自己的数据平台可以直接基于页面框架进行改造
  * 优点：原装 Hadoop、纯开源、支持 tez(分布式dh框架)
  * 缺点：企业级安全不开源，
* MapReduce

## HDFS Architecture

* NameNode and DataNodes
* master/slave architecture

### 架构概述

HDFS has a **`master/slave`** architecture. An HDFS cluster consists of a single NameNode, a master server that **`manages the file system namespace`** and **`regulates access to files by clients`**.

In addition, there are a number of DataNodes, usually one per node in the cluster, which **`manage storage attached to the nodes`** that they are run on.

HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file split into one or more blocks and these blocks are stored in a set of DataNodes(一组 DataNode).

**`The NameNode executes file system namespace operations like opening, closing, and renaming files and directories`**. It also determines the mapping of blocks to DataNodes.

**`The DataNodes are responsible for serving read and write requests from the file system's clients. The DataNodes also perform block creation、deletion、and replication upon instruction from the NameNode`**.

A typical deployment has a dedicated(专有的) machine that runs only **`the NameNode software`**. Each of the other machines in the cluster runs **`one instance of the DataNode software`**. The architecture does not preclude(排斥) running multiple DataNodes on the same machine but in a real deployment that is rarely the case.

### The File System Namespace

HDFS supports a traditional hierarchical(层级的) file organization. A user or an application can create directories and store files inside these directories. **`The file system namespace hierarchy is similar to most other existing file systems`**; one can create and remove files, move a file from one directory to another, or rename a file. HDFS supports **user quotas** and **access permissions**. HDFS does not support hard links or soft links. However, the HDFS architecture does not preclude implementing these features.

**`The NameNode maintains the file system namespace. Any change to the file system namespace or its properties is recorded by the HDFS`**. An application can specify the number of replicas of a file that should be maintained by HDFS. The number of copies of a file is called the **replication factor** of that file. This information is stored by the NameNode.

### Data Replication

HDFS is designed to reliably store very **large files** across machines in a large cluster. It stores each file as a sequence of blocks. **The blocks of file are replicated for fault tolerance**. The block size and replication factor are configurable per file.

All blocks in a file except the last block are the same size, while users can start a new block without filling out the last block to the configured block size after the support for variable length block was added to append and hsync.(同时在将对可变长度 block 的支持添加到“append”和“hsync“功能中后，用户可以以一个新的 block 开始而不必将原有最后一个block 填满至配置的容量规格)

An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once(except for appends and truncates 除过追加和截断) and have strictly(严格地、确实地) one writer at any time.

The NameNode makes all decisions regarding replication of blocks. It periodically receives a **Heartbeat** and a **Blockreport** from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode.

![hdfsdatanodes](/assets/hdfsdatanodes.png)

## linux 操作

linux 用户名：hadoop
密码：123456
HostName: hadoop000

创建文件夹：
    mkdir software 软件安装包存放位置
    mkdir app      软件安装位置
    mkdir data     数据存放位置
    mkdir lib      存放作业 jar
    mkdir shell    存放相关脚本
    mkdir maven_resp 存放 maven依赖包
hadoop 用户转 root 用户：
    ```sudo -i```
反向
    ```su hadoop```
linux 版本：centos7
    ```uname -a```

配置好 IP 及域名映射：
    ```vim /etc/hosts```
    server ip   hostname

## 版本选择

下载地址：http://archive.cloudera.com/cdh5/cdh/5/
Hadoop 选择hadoop-2.6.0-cdh5.15.1.tar.gz

使用单机版进项框架学习

Hadoop 安装前置要求
    Java 1.8+
    ssh

将本地文件拷贝到服务器上
    scp命令
解压
    tar -zvxf
配置 Java 环境变量 $JAVA_HOME 与 PATH
生效
    source

## hadoop 安装

$解压$

``` bash
tar -zxvf -C ~/app/
```

添加 HADOOP_HOME/bin 到系统环境变量文件
修改 Hadoop 配置文件--hadoop-env.sh-java_home

hadoop 软件包常用目录名单
    bin: hadoop 客户端名单
    etc/hadoop：hadoop相关的配置文件存放目录
    sbin: 启动 hadoop相关进程的脚本


为分布模式配置

etc/hadoop/core-site.xml:

``` bash
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:8020</value>
    </property>
</configuration>
```

etc/hadoop/hdfs-site.xml:

``` bash
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

hdfs-site.xml中还需要配置
hadoop.tmp.dir	/tmp/hadoop-${user.name}

在 slaves 文件中添加本机域名即为 hadoop000

$启动 HDFS$
1. 格式化文件系统
2. 启动 namenode 和 DataNode
3. 使用 jps命令进行验证 或者使用 http://ip:50070
4. 使用`sudo systemctl stop firewall.service`命令关闭防火墙，可以修改为禁止防火墙开机启动

查看防火墙状态

``` bash
firewall-cmd --state
```

